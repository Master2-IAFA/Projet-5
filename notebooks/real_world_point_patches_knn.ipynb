{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-q6l6z_5m because the default path (/home/user/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from glob import glob\n",
    "from io import StringIO, BytesIO\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import h5py\n",
    "import k3d\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pymesh\n",
    "import torch\n",
    "import trimesh.transformations as tt\n",
    "import trimesh\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sharpness(mesh=None, plot_meshvert=True,\n",
    "                      samples=None, samples_distances=None,\n",
    "                      sharp_vert=None, sharp_curves=None,\n",
    "                      directions=None, directions_width=0.0025,\n",
    "                      samples_color=0x0000ff, samples_psize=0.002, \n",
    "                      mesh_color=0xbbbbbb, meshvert_color=0x666666, meshvert_psize=0.0025,\n",
    "                      sharpvert_color=0xff0000, sharpvert_psize=0.0025,\n",
    "                      sharpcurve_color=None, sharpcurve_width=0.0025,\n",
    "                      as_image=False, plot_height=768,\n",
    "                      cmap=k3d.colormaps.matplotlib_color_maps.coolwarm_r):\n",
    "    \n",
    "    plot = k3d.plot(height=plot_height)\n",
    "    \n",
    "    if None is not mesh:\n",
    "        k3d_mesh = k3d.mesh(mesh.vertices, mesh.faces, color=mesh_color)\n",
    "        plot += k3d_mesh\n",
    "\n",
    "        if plot_meshvert:\n",
    "            k3d_points = k3d.points(mesh.vertices, \n",
    "                                    point_size=meshvert_psize, color=meshvert_color)\n",
    "            plot += k3d_points\n",
    "            k3d_points.shader='flat'\n",
    "\n",
    "    if None is not samples:\n",
    "        colors = None\n",
    "        if None is not samples_distances:\n",
    "            max_dist = 1.0\n",
    "\n",
    "            colors = k3d.helpers.map_colors(\n",
    "                samples_distances, cmap, [0, max_dist]\n",
    "            ).astype(np.uint32)\n",
    "            k3d_points = k3d.points(samples, point_size=samples_psize, colors=colors)\n",
    "        else:\n",
    "            if samples_color == 'z':\n",
    "                v = -np.array([0., 0., 1.])\n",
    "                max_dist = np.max(np.dot(samples, v))\n",
    "                min_dist = np.min(np.dot(samples, v))\n",
    "                colors = k3d.helpers.map_colors(\n",
    "                    np.dot(samples, v), k3d.colormaps.matplotlib_color_maps.viridis, [min_dist, max_dist]\n",
    "                ).astype(np.uint32)\n",
    "                k3d_points = k3d.points(samples, point_size=samples_psize, colors=colors)\n",
    "                \n",
    "            else:\n",
    "                k3d_points = k3d.points(samples, point_size=samples_psize, color=samples_color)\n",
    "                \n",
    "        plot += k3d_points\n",
    "        k3d_points.shader='flat'\n",
    "        \n",
    "        if None is not directions:\n",
    "            vectors = k3d.vectors(\n",
    "                samples,\n",
    "                directions * samples_distances[..., np.newaxis],\n",
    "                use_head=False, \n",
    "                line_width=directions_width)\n",
    "            print(vectors)\n",
    "            plot += vectors\n",
    "\n",
    "#             directions_to_plot = np.hstack((samples, samples + directions))\n",
    "            \n",
    "#             for i, dir_to_plot in enumerate(directions_to_plot):\n",
    "#                 dir_to_plot = dir_to_plot.reshape((2, 3))\n",
    "#                 if np.all(dir_to_plot[0] == dir_to_plot[1]):\n",
    "#                     continue\n",
    "#                 color = int(colors[i]) if None is not colors else samples_color\n",
    "#                 plt_line = k3d.line(dir_to_plot, \n",
    "#                                     shader='mesh', width=directions_width, color=color)\n",
    "#                 plot += plt_line\n",
    "\n",
    "    if None is not sharp_vert:\n",
    "        k3d_points = k3d.points(sharp_vert,\n",
    "                                point_size=sharpvert_psize, color=sharpvert_color)\n",
    "        plot += k3d_points\n",
    "        k3d_points.shader='flat'\n",
    "        \n",
    "        if None is not sharp_curves:            \n",
    "            if None is not sharpcurve_color:\n",
    "                color = sharpcurve_color\n",
    "            else:\n",
    "                import randomcolor\n",
    "                rand_color = randomcolor.RandomColor()\n",
    "            for i, vert_ind in enumerate(sharp_curves):\n",
    "                sharp_points_curve = mesh.vertices[vert_ind]\n",
    "                \n",
    "                if None is sharpcurve_color:\n",
    "                    color = rand_color.generate(hue='red')[0]\n",
    "                    color = int('0x' + color[1:], 16)\n",
    "                plt_line = k3d.line(sharp_points_curve, \n",
    "                                    shader='mesh', width=sharpcurve_width, color=color)\n",
    "                plot += plt_line\n",
    "        \n",
    "    plot.grid_visible = False\n",
    "    plot.display()\n",
    "    \n",
    "    if as_image:\n",
    "        plot.fetch_screenshot()\n",
    "        return Image(data=b64decode(plot.screenshot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm/pixel\n",
    "HIGH_RES = 0.02\n",
    "MED_RES = 0.05\n",
    "LOW_RES = 0.125\n",
    "XLOW_RES = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with point patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharpf.data import DataGenerationException\n",
    "from sharpf.utils.abc_utils.abc.abc_data import ABCModality, ABCChunk, ABC_7Z_FILEMASK\n",
    "from sharpf.data.annotation import ANNOTATOR_BY_TYPE\n",
    "from sharpf.data.camera_pose_manager import POSE_MANAGER_BY_TYPE\n",
    "from sharpf.data.imaging import IMAGING_BY_TYPE\n",
    "from sharpf.data.noisers import NOISE_BY_TYPE\n",
    "from sharpf.utils.abc_utils.abc import feature_utils\n",
    "from sharpf.utils.py_utils.console import eprint_t\n",
    "from sharpf.utils.py_utils.os import add_suffix\n",
    "from sharpf.utils.py_utils.config import load_func_from_config\n",
    "from sharpf.utils.abc_utils.mesh.io import trimesh_load\n",
    "from sharpf.utils.plotting import display_depth_sharpness\n",
    "from sharpf.utils.camera_utils.camera_pose import camera_to_display\n",
    "from sharpf.utils.abc_utils.mesh.indexing import reindex_zerobased, compute_relative_indexes\n",
    "import sharpf.data.data_smells as smells\n",
    "import sharpf.utils.abc_utils.hdf5.io_struct as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# https://github.com/corochann/chainer-pointnet/blob/master/chainer_pointnet/utils/sampling.py\n",
    "\n",
    "def l2_norm(x, y):\n",
    "    \"\"\"Calculate l2 norm (distance) of `x` and `y`.\n",
    "    Args:\n",
    "        x (numpy.ndarray or cupy): (batch_size, num_point, coord_dim)\n",
    "        y (numpy.ndarray): (batch_size, num_point, coord_dim)\n",
    "    Returns (numpy.ndarray): (batch_size, num_point,)\n",
    "    \"\"\"\n",
    "    return ((x - y) ** 2).sum(axis=2)\n",
    "\n",
    "\n",
    "def farthest_point_sampling(pts, k, initial_idx=None, metrics=l2_norm,\n",
    "                            skip_initial=False, indices_dtype=numpy.int32,\n",
    "                            distances_dtype=numpy.float32):\n",
    "    \"\"\"Batch operation of farthest point sampling\n",
    "    Code referenced from below link by @Graipher\n",
    "    https://codereview.stackexchange.com/questions/179561/farthest-point-algorithm-in-python\n",
    "    Args:\n",
    "        pts (numpy.ndarray or cupy.ndarray): 2-dim array (num_point, coord_dim)\n",
    "            or 3-dim array (batch_size, num_point, coord_dim)\n",
    "            When input is 2-dim array, it is treated as 3-dim array with\n",
    "            `batch_size=1`.\n",
    "        k (int): number of points to sample\n",
    "        initial_idx (int): initial index to start farthest point sampling.\n",
    "            `None` indicates to sample from random index,\n",
    "            in this case the returned value is not deterministic.\n",
    "        metrics (callable): metrics function, indicates how to calc distance.\n",
    "        skip_initial (bool): If True, initial point is skipped to store as\n",
    "            farthest point. It stabilizes the function output.\n",
    "        xp (numpy or cupy):\n",
    "        indices_dtype (): dtype of output `indices`\n",
    "        distances_dtype (): dtype of output `distances`\n",
    "    Returns (tuple): `indices` and `distances`.\n",
    "        indices (numpy.ndarray or cupy.ndarray): 2-dim array (batch_size, k, )\n",
    "            indices of sampled farthest points.\n",
    "            `pts[indices[i, j]]` represents `i-th` batch element of `j-th`\n",
    "            farthest point.\n",
    "        distances (numpy.ndarray or cupy.ndarray): 3-dim array\n",
    "            (batch_size, k, num_point)\n",
    "    \"\"\"\n",
    "    if pts.ndim == 2:\n",
    "        # insert batch_size axis\n",
    "        pts = pts[None, ...]\n",
    "    assert pts.ndim == 3\n",
    "    xp = np\n",
    "    batch_size, num_point, coord_dim = pts.shape\n",
    "    indices = xp.zeros((batch_size, k, ), dtype=indices_dtype)\n",
    "\n",
    "    # distances[bs, i, j] is distance between i-th farthest point `pts[bs, i]`\n",
    "    # and j-th input point `pts[bs, j]`.\n",
    "    distances = xp.zeros((batch_size, k, num_point), dtype=distances_dtype)\n",
    "    if initial_idx is None:\n",
    "        indices[:, 0] = xp.random.randint(len(pts))\n",
    "    else:\n",
    "        indices[:, 0] = initial_idx\n",
    "\n",
    "    batch_indices = xp.arange(batch_size)\n",
    "    farthest_point = pts[batch_indices, indices[:, 0]]\n",
    "    # minimum distances to the sampled farthest point\n",
    "    try:\n",
    "        min_distances = metrics(farthest_point[:, None, :], pts)\n",
    "    except Exception as e:\n",
    "        import IPython; IPython.embed()\n",
    "\n",
    "    if skip_initial:\n",
    "        # Override 0-th `indices` by the farthest point of `initial_idx`\n",
    "        indices[:, 0] = xp.argmax(min_distances, axis=1)\n",
    "        farthest_point = pts[batch_indices, indices[:, 0]]\n",
    "        min_distances = metrics(farthest_point[:, None, :], pts)\n",
    "\n",
    "    distances[:, 0, :] = min_distances\n",
    "    for i in tqdm(range(1, k)):\n",
    "        indices[:, i] = xp.argmax(min_distances, axis=1)\n",
    "        farthest_point = pts[batch_indices, indices[:, i]]\n",
    "        dist = metrics(farthest_point[:, None, :], pts)\n",
    "        distances[:, i, :] = dist\n",
    "        min_distances = xp.minimum(min_distances, dist)\n",
    "    return indices, distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_using_bfs(nn_indexes, centroid_idx):\n",
    "    patch_points = {centroid_idx}\n",
    "    stack = deque()\n",
    "    stack.append(centroid_idx)\n",
    "    \n",
    "    while len(stack) > 0 and len(patch_points) < 4096:\n",
    "        point_idx = stack.popleft()        \n",
    "        for i in nn_indexes[point_idx]:\n",
    "            if i not in patch_points and i not in stack:\n",
    "                patch_points.add(point_idx)\n",
    "                stack.append(i)\n",
    "#         print(len(stack), len(patch_points))\n",
    "\n",
    "    return np.array(list(patch_points))\n",
    "\n",
    "\n",
    "def patches_from_point_cloud(points, n_patches):\n",
    "    # spread patch centroids across the point clouds\n",
    "    inds, ds = farthest_point_sampling(points, k=n_patches)\n",
    "    \n",
    "    # get kNN graph from point cloud\n",
    "    tree = cKDTree(points, leafsize=1024)\n",
    "    nn_distances, nn_indexes = tree.query(points, k=5, n_jobs=10)\n",
    "    \n",
    "    # startting from each seed centroid, traverse kNN graph and yield patches (slow)\n",
    "    for centroid_idx in tqdm(inds[0]):\n",
    "        yield patch_using_bfs(nn_indexes, centroid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(base_dir):\n",
    "    matlab_project_filename = glob(os.path.join(base_dir, '*.mlp'))[0]\n",
    "    root = ET.parse(matlab_project_filename).getroot()\n",
    "\n",
    "    points_by_scan = []\n",
    "    transform_by_scan_4x4 = []\n",
    "\n",
    "    item_id = None\n",
    "    for type_tag in root.findall('MeshGroup/MLMesh'):\n",
    "        filename = type_tag.get('filename')\n",
    "        if filename.endswith('.obj'):\n",
    "            item_id = filename\n",
    "\n",
    "        elif filename.endswith('.ply'):\n",
    "            try:\n",
    "                transform = np.loadtxt(\n",
    "                    StringIO(type_tag.find('MLMatrix44').text))\n",
    "                points = trimesh.load(os.path.join(base_dir, filename)).vertices\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "            \n",
    "            transform_by_scan_4x4.append(transform)\n",
    "            points_by_scan.append(points)\n",
    "            \n",
    "    return points_by_scan, transform_by_scan_4x4, item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnlabeledPointCloudIO = io.HDF5IO({\n",
    "        'points': io.Float64('points'),\n",
    "        'indexes_in_whole': io.Int32('indexes_in_whole'),\n",
    "        'distances': io.Float64('distances'),\n",
    "        'item_id': io.AsciiString('item_id'),\n",
    "    },\n",
    "    len_label='has_sharp',\n",
    "    compression='lzf')\n",
    "\n",
    "\n",
    "def save_point_crops(patches, filename):\n",
    "    # turn a list of dicts into a dict of torch tensors:\n",
    "    # default_collate([{'a': 'str1', 'x': np.random.normal()}, {'a': 'str2', 'x': np.random.normal()}])\n",
    "    # Out[26]: {'a': ['str1', 'str2'], 'x': tensor([0.4252, 0.1414], dtype=torch.float64)}\n",
    "    collate_fn = partial(io.collate_mapping_with_io, io=UnlabeledPointCloudIO)\n",
    "    patches = collate_fn(patches)\n",
    "\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for key in ['points', 'indexes_in_whole', 'distances']:\n",
    "            UnlabeledPointCloudIO.write(f, key, patches[key].numpy())\n",
    "        UnlabeledPointCloudIO.write(f, 'item_id', patches['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scans(input_dir, output_filename):\n",
    "    points_by_scan, transform_by_scan_4x4, item_id = load_scans(input_dir)\n",
    "    \n",
    "    all_points = np.concatenate([\n",
    "        tt.transform_points(points, transform)\n",
    "        for points, transform in zip(points_by_scan, transform_by_scan_4x4)])\n",
    "    n_patches = int(len(all_points) * 10 / 4096)\n",
    "    print('n_patches = ', n_patches)\n",
    "    \n",
    "    point_patches = []\n",
    "    for patch_point_indexes in patches_from_point_cloud(all_points, n_patches):\n",
    "        if len(patch_point_indexes) == 4096:\n",
    "            point_patches.append({\n",
    "                'points': all_points[patch_point_indexes],\n",
    "                'distances': np.ones(4096),\n",
    "                'indexes_in_whole': patch_point_indexes,\n",
    "                'item_id': item_id\n",
    "            })\n",
    "            \n",
    "    save_point_crops(point_patches, output_filename)\n",
    "    return all_points, point_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = !find /data/abc/sharp_features_whole_models/SL -maxdepth 1 -mindepth 1 -type d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo = [os.path.basename(d) for d in s\n",
    "       if not os.path.exists(\n",
    "           '/data/abc/sharp_features_whole_models/SL/{}/{}.hdf5'.format(os.path.basename(d), os.path.basename(d)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "todo = [\n",
    "#     'smallest_345',\n",
    " 'bat_320',\n",
    " 'smallest_40',\n",
    " 'small_40',\n",
    " 'cross_40',\n",
    " 'Detail_45',\n",
    " 'car_320',\n",
    " 'cube_small_40',\n",
    " 'hook_40cm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat_320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 548.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_patches =  93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 93/93 [00:09<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smallest_40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 27/198 [00:00<00:00, 266.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_patches =  199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 321.39it/s]\n",
      "100%|██████████| 199/199 [00:25<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1083 [00:00<00:16, 63.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_patches =  1084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1083/1083 [00:17<00:00, 61.39it/s]\n",
      "100%|██████████| 1084/1084 [02:19<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_40\n",
      "n_patches =  7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7324/7324 [15:11<00:00,  8.03it/s]\n",
      "100%|██████████| 7325/7325 [17:20<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail_45\n",
      "n_patches =  28178\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.18 TiB for an array with shape (1, 28178, 11541725) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-224-d56bf870cc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     all_points, point_patches = process_scans(\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'/data/abc/sharp_features_whole_models/SL/{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m'/data/abc/sharp_features_whole_models/SL/{}/{}.hdf5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-163-112ceb2544b4>\u001b[0m in \u001b[0;36mprocess_scans\u001b[0;34m(input_dir, output_filename)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpoint_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpatch_point_indexes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpatches_from_point_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_point_indexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             point_patches.append({\n",
      "\u001b[0;32m<ipython-input-154-b7c83155cec3>\u001b[0m in \u001b[0;36mpatches_from_point_cloud\u001b[0;34m(points, n_patches)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpatches_from_point_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# spread patch centroids across the point clouds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfarthest_point_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# get kNN graph from point cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-68b3c8423cb2>\u001b[0m in \u001b[0;36mfarthest_point_sampling\u001b[0;34m(pts, k, initial_idx, metrics, skip_initial, indices_dtype, distances_dtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# distances[bs, i, j] is distance between i-th farthest point `pts[bs, i]`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# and j-th input point `pts[bs, j]`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.18 TiB for an array with shape (1, 28178, 11541725) and data type float32"
     ]
    }
   ],
   "source": [
    "for name in todo:\n",
    "    print(name)\n",
    "    all_points, point_patches = process_scans(\n",
    "        '/data/abc/sharp_features_whole_models/SL/{}/'.format(name),\n",
    "        '/data/abc/sharp_features_whole_models/SL/{}/{}.hdf5'.format(name, name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points_by_scan, transform_by_scan_4x4, item_id = load_scans('/data/abc/sharp_features_whole_models/SL/Castle_45/')\n",
    "\n",
    "# all_points = np.concatenate([\n",
    "#     tt.transform_points(points, transform)\n",
    "#     for points, transform in zip(points_by_scan, transform_by_scan_4x4)])\n",
    "\n",
    "# display_sharpness(samples=all_points, samples_psize=0.1, samples_color='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File /data/abc/sharp_features_whole_models/SL/Cross_345/Cross_345.hdf5 is not compatible with Hdf5File I/O interface <class 'sharpf.utils.abc_utils.hdf5.io_struct.HDF5IO'>\n",
      "File /data/abc/sharp_features_whole_models/SL/Cross_345/Cross_345.hdf5 is not compatible with Hdf5File I/O interface <class 'sharpf.utils.abc_utils.hdf5.io_struct.HDF5IO'>\n",
      "77it [00:00, 6802.77it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380addc7acf4466a8f9f4cd0a78082c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sharpf.utils.abc_utils.hdf5.dataset import Hdf5File, PreloadTypes \n",
    "\n",
    "ground_truth = Hdf5File('/data/abc/sharp_features_whole_models/SL/Cross_345/Cross_345.hdf5',\n",
    "                   io=UnlabeledPointCloudIO,\n",
    "                   preload=PreloadTypes.LAZY,\n",
    "                   labels='*')\n",
    "\n",
    "\n",
    "n_points = np.concatenate([patch['indexes_in_whole'] for patch in ground_truth]).max() + 1\n",
    "whole_model_points_gt = np.zeros((n_points, 3))\n",
    "whole_model_distances_gt = np.ones(n_points) * np.inf\n",
    "\n",
    "for patch in tqdm(ground_truth):\n",
    "    distances = patch['distances']\n",
    "    indexes = patch['indexes_in_whole']\n",
    "    whole_model_points_gt[indexes] = patch['points'].reshape((-1, 3))\n",
    "\n",
    "    assign_mask = whole_model_distances_gt[indexes] > distances\n",
    "    whole_model_distances_gt[indexes[assign_mask]] = np.minimum(distances[assign_mask], 1.0)\n",
    "\n",
    "\n",
    "    \n",
    "display_sharpness(samples=whole_model_points_gt, samples_psize=0.1, samples_color='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = trimesh.load('/data/abc/sharp_features_whole_models/scan_res_0000/AlexeyDetail_Big 1.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrackedArray([[  -1.06496   , -143.62800598,   93.68890381],\n",
       "              [  -5.25204992, -137.04899597,   93.61959839],\n",
       "              [ -30.73900032, -142.18400574,   79.54850006],\n",
       "              ...,\n",
       "              [ -31.39830017, -138.03300476,   78.66459656],\n",
       "              [   5.01087999, -151.92399597,   91.376297  ],\n",
       "              [ -29.00819969, -160.59500122,   81.30799866]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan.vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42d815d0f1c48f0b2b7b578d27496de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sharpness(\n",
    "    samples=scan.vertices,\n",
    "    samples_color='z',\n",
    "    samples_psize=0.05,\n",
    "    sharp_vert=[[0.,0.,0.0]],\n",
    "    sharpvert_psize=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.loadtxt('/data/abc/sharp_features_whole_models/scan_res_0000/vertex_matrix.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.978958, -3.72529e-09, 0.20406, 0.0],\n",
       " [-0.0573489, 0.959696, 0.275126, 0.0],\n",
       " [-0.195835, -0.28104, 0.939503, 0.0],\n",
       " [0.0, 0.0, 0.0, 1.0]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/abc/sharp_features_whole_models/scan_res_0000/Raw/impara01.txt') as f:\n",
    "    impara01 = f.read().strip().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = np.array([0.22859, 0.188937, -0.000396054])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.array([-233.672, -335.338, 1121.02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix(angles):\n",
    "    angle_x, angle_y, angle_z = angles\n",
    "    e_x, e_y, e_z = np.eye(3)\n",
    "\n",
    "    rotation_x = tt.rotation_matrix(angle_x, e_x)\n",
    "    rotation_y = tt.rotation_matrix(angle_y, e_y)\n",
    "    rotation_z = tt.rotation_matrix(angle_z, e_z)\n",
    "\n",
    "    rotation = np.dot(\n",
    "        np.dot(rotation_x, rotation_y),\n",
    "        rotation_z)\n",
    "\n",
    "    return rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T = np.dot(\n",
    "    rotation_matrix(angles),\n",
    "    tt.translation_matrix(X0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80af33fb6f684b2cb6179b3823abc8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_sharpness(\n",
    "    samples=tt.transform_points(scan.vertices, T),\n",
    "    samples_color='z',\n",
    "    samples_psize=0.05,\n",
    "    sharp_vert=[X0],\n",
    "    sharpvert_psize=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
